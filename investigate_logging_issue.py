#!/usr/bin/env python3
"""
PyTorch Lightning é‡å¤æ—¥å¿—é—®é¢˜è°ƒæŸ¥è„šæœ¬
ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ¥åˆ†æä½ çš„model.pyæ–‡ä»¶ä¸­çš„é‡å¤loggingé—®é¢˜
"""

import os
import re
import sys
from pathlib import Path

def find_model_files():
    """æŸ¥æ‰¾å¯èƒ½çš„model.pyæ–‡ä»¶"""
    print("ğŸ” æœç´¢model.pyæ–‡ä»¶...")
    
    possible_paths = [
        "/home/sunx/data/aiiih/projects/sunx/projects/SeqSetVAE/main/model.py",
        "/data/aiiih/projects/sunx/projects/SeqSetVAE/main/model.py", 
    ]
    
    # ä¹Ÿæœç´¢å½“å‰ç”¨æˆ·ç›®å½•
    home_dir = os.path.expanduser("~")
    for root, dirs, files in os.walk(home_dir):
        if "model.py" in files and "SeqSetVAE" in root:
            possible_paths.append(os.path.join(root, "model.py"))
        # é™åˆ¶æœç´¢æ·±åº¦é¿å…å¤ªæ…¢
        if root.count(os.sep) - home_dir.count(os.sep) > 5:
            dirs.clear()
    
    found_files = []
    for path in possible_paths:
        if os.path.exists(path):
            found_files.append(path)
            print(f"âœ… æ‰¾åˆ°: {path}")
    
    if not found_files:
        print("âŒ æœªæ‰¾åˆ°model.pyæ–‡ä»¶")
        print("è¯·æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶è·¯å¾„:")
        manual_path = input("è¾“å…¥model.pyçš„å®Œæ•´è·¯å¾„: ").strip()
        if os.path.exists(manual_path):
            found_files.append(manual_path)
        else:
            print("âŒ æŒ‡å®šçš„æ–‡ä»¶ä¸å­˜åœ¨")
            return []
    
    return found_files

def analyze_logging_calls(file_path):
    """åˆ†ææ–‡ä»¶ä¸­çš„æ‰€æœ‰loggingè°ƒç”¨"""
    print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {file_path}")
    print("=" * 80)
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {e}")
        return
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰self.logè°ƒç”¨
    log_calls = find_all_log_calls(content)
    
    # 2. åˆ†æfocal_lossç›¸å…³çš„è°ƒç”¨
    focal_loss_calls = [call for call in log_calls if 'focal_loss' in call['content'].lower()]
    
    print(f"ğŸ“ˆ æ€»å…±æ‰¾åˆ° {len(log_calls)} ä¸ªloggingè°ƒç”¨")
    print(f"ğŸ¯ å…¶ä¸­ {len(focal_loss_calls)} ä¸ªä¸focal_lossç›¸å…³")
    
    if focal_loss_calls:
        print("\nğŸš¨ focal_lossç›¸å…³çš„loggingè°ƒç”¨:")
        print("-" * 50)
        for call in focal_loss_calls:
            print(f"ç¬¬ {call['line']} è¡Œ: {call['type']}")
            print(f"   å†…å®¹: {call['content'][:100]}{'...' if len(call['content']) > 100 else ''}")
            print()
    
    # 3. åˆ†ævalidation_stepæ–¹æ³•
    analyze_validation_step(content)
    
    # 4. æ£€æŸ¥å¯èƒ½çš„é‡å¤æ¨¡å¼
    check_duplicate_patterns(log_calls)
    
    # 5. æ£€æŸ¥æœ€è¿‘å¯èƒ½çš„é—®é¢˜ä»£ç 
    check_problematic_patterns(content)

def find_all_log_calls(content):
    """æŸ¥æ‰¾æ‰€æœ‰çš„loggingè°ƒç”¨"""
    log_calls = []
    
    # åŒ¹é…self.logå’Œself.log_dictè°ƒç”¨
    patterns = [
        (r'self\.log\s*\([^)]*\)', 'self.log'),
        (r'self\.log_dict\s*\([^)]*\)', 'self.log_dict')
    ]
    
    for pattern, call_type in patterns:
        matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)
        for match in matches:
            line_num = content[:match.start()].count('\n') + 1
            log_calls.append({
                'line': line_num,
                'type': call_type,
                'content': match.group(0),
                'start_pos': match.start(),
                'end_pos': match.end()
            })
    
    # æŒ‰è¡Œå·æ’åº
    log_calls.sort(key=lambda x: x['line'])
    return log_calls

def analyze_validation_step(content):
    """åˆ†ævalidation_stepæ–¹æ³•"""
    print("\nğŸ”¬ åˆ†ævalidation_stepæ–¹æ³•:")
    print("-" * 50)
    
    # æŸ¥æ‰¾validation_stepæ–¹æ³•
    validation_pattern = r'def\s+validation_step\s*\([^)]*\):(.*?)(?=def\s+\w+|class\s+\w+|\Z)'
    match = re.search(validation_pattern, content, re.DOTALL)
    
    if match:
        method_content = match.group(1)
        method_start = match.start()
        
        # åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æŸ¥æ‰¾loggingè°ƒç”¨
        log_calls_in_method = []
        log_patterns = [
            r'self\.log\s*\([^)]*\)',
            r'self\.log_dict\s*\([^)]*\)'
        ]
        
        for pattern in log_patterns:
            for log_match in re.finditer(pattern, method_content, re.MULTILINE | re.DOTALL):
                line_in_method = method_content[:log_match.start()].count('\n') + 1
                total_line = content[:method_start].count('\n') + line_in_method + 1
                log_calls_in_method.append({
                    'line': total_line,
                    'content': log_match.group(0)
                })
        
        if log_calls_in_method:
            print(f"åœ¨validation_stepæ–¹æ³•ä¸­æ‰¾åˆ° {len(log_calls_in_method)} ä¸ªloggingè°ƒç”¨:")
            for call in log_calls_in_method:
                print(f"  ç¬¬ {call['line']} è¡Œ: {call['content'][:80]}{'...' if len(call['content']) > 80 else ''}")
        else:
            print("validation_stepæ–¹æ³•ä¸­æ²¡æœ‰ç›´æ¥çš„loggingè°ƒç”¨")
            
        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†_stepæ–¹æ³•
        if '_step(' in method_content:
            print("âš ï¸  validation_stepè°ƒç”¨äº†_stepæ–¹æ³•ï¼Œéœ€è¦æ£€æŸ¥_stepä¸­çš„logging")
    else:
        print("âŒ æœªæ‰¾åˆ°validation_stepæ–¹æ³•")

def check_duplicate_patterns(log_calls):
    """æ£€æŸ¥å¯èƒ½å¯¼è‡´é‡å¤çš„æ¨¡å¼"""
    print("\nğŸ” æ£€æŸ¥é‡å¤æ¨¡å¼:")
    print("-" * 50)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªlog_dictè°ƒç”¨
    log_dict_calls = [call for call in log_calls if 'log_dict' in call['type']]
    if len(log_dict_calls) > 1:
        print(f"âš ï¸  å‘ç° {len(log_dict_calls)} ä¸ªlog_dictè°ƒç”¨ï¼Œå¯èƒ½åŒ…å«é‡å¤é”®:")
        for call in log_dict_calls:
            print(f"  ç¬¬ {call['line']} è¡Œ")
    
    # æ£€æŸ¥æ˜¯å¦æ··åˆä½¿ç”¨logå’Œlog_dict
    log_calls_simple = [call for call in log_calls if call['type'] == 'self.log']
    if log_calls_simple and log_dict_calls:
        print(f"âš ï¸  åŒæ—¶ä½¿ç”¨äº†self.log ({len(log_calls_simple)}æ¬¡) å’Œ self.log_dict ({len(log_dict_calls)}æ¬¡)")
        print("   è¿™å¯èƒ½å¯¼è‡´ç›¸åŒé”®è¢«è®°å½•å¤šæ¬¡")
    
    # æ£€æŸ¥è¡Œå·æ¥è¿‘çš„è°ƒç”¨
    for i in range(len(log_calls) - 1):
        current = log_calls[i]
        next_call = log_calls[i + 1]
        if next_call['line'] - current['line'] <= 5:  # 5è¡Œå†…çš„è°ƒç”¨
            print(f"âš ï¸  ç¬¬ {current['line']} è¡Œå’Œç¬¬ {next_call['line']} è¡Œçš„loggingè°ƒç”¨å¾ˆæ¥è¿‘")

def check_problematic_patterns(content):
    """æ£€æŸ¥å¯èƒ½æœ‰é—®é¢˜çš„ä»£ç æ¨¡å¼"""
    print("\nğŸš¨ æ£€æŸ¥é—®é¢˜æ¨¡å¼:")
    print("-" * 50)
    
    problematic_patterns = [
        (r'self\.log.*focal_loss.*\n.*self\.log.*focal_loss', "åŒä¸€åŒºåŸŸå¤šæ¬¡è®°å½•focal_loss"),
        (r'val_dataloader.*\[.*\]', "å¤šä¸ªéªŒè¯æ•°æ®åŠ è½½å™¨"),
        (r'for.*fold.*in', "å¯èƒ½çš„äº¤å‰éªŒè¯å¾ªç¯"),
        (r'super\(\)\.validation_step', "è°ƒç”¨çˆ¶ç±»validation_step"),
        (r'splitdata|split_data', "æ•°æ®åˆ†å‰²ç›¸å…³ä»£ç "),
    ]
    
    found_issues = []
    for pattern, description in problematic_patterns:
        matches = list(re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE))
        if matches:
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                found_issues.append((line_num, description, match.group(0)[:50]))
    
    if found_issues:
        print("å‘ç°å¯èƒ½çš„é—®é¢˜æ¨¡å¼:")
        for line, desc, match_text in found_issues:
            print(f"  ç¬¬ {line} è¡Œ: {desc}")
            print(f"    ä»£ç ç‰‡æ®µ: {match_text}...")
    else:
        print("âœ… æœªå‘ç°æ˜æ˜¾çš„é—®é¢˜æ¨¡å¼")

def provide_specific_fix_suggestions(file_path):
    """æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®"""
    print(f"\nğŸ’¡ é’ˆå¯¹ {file_path} çš„ä¿®å¤å»ºè®®:")
    print("=" * 80)
    
    print("""
1. ç«‹å³ä¿®å¤æ­¥éª¤:
   - æœç´¢æ‰€æœ‰åŒ…å« 'val/focal_loss' çš„è¡Œ
   - ç¡®ä¿åªåœ¨ä¸€ä¸ªåœ°æ–¹è®°å½•è¿™ä¸ªæŒ‡æ ‡
   - æ£€æŸ¥validation_stepå’Œ_stepæ–¹æ³•ä¸­çš„æ‰€æœ‰loggingè°ƒç”¨

2. æ¨èçš„ä¿®å¤æ¨¡å¼:
   ```python
   def validation_step(self, batch, batch_idx):
       # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
       logits, recon_loss, kl_loss = self(batch)
       focal_loss = self.compute_focal_loss(logits, batch)
       
       # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡åˆ°ä¸€ä¸ªå­—å…¸
       val_metrics = {
           'val/focal_loss': focal_loss,
           'val/recon_loss': recon_loss,
           'val/kl_loss': kl_loss,
       }
       
       # ä¸€æ¬¡æ€§è®°å½•æ‰€æœ‰æŒ‡æ ‡
       self.log_dict(val_metrics, prog_bar=True, sync_dist=True)
       
       return focal_loss
   ```

3. è°ƒè¯•å‘½ä»¤:
   ```bash
   grep -n "val/focal_loss" """ + file_path + """
   grep -n "self.log" """ + file_path + """ | head -20
   ```
""")

def main():
    print("ğŸ”§ PyTorch Lightning é‡å¤æ—¥å¿—é—®é¢˜è°ƒæŸ¥å·¥å…·")
    print("=" * 80)
    
    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    model_files = find_model_files()
    
    if not model_files:
        print("âŒ æ— æ³•æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè°ƒæŸ¥ç»ˆæ­¢")
        return
    
    # åˆ†ææ¯ä¸ªæ‰¾åˆ°çš„æ–‡ä»¶
    for file_path in model_files:
        analyze_logging_calls(file_path)
        provide_specific_fix_suggestions(file_path)
        print("\n" + "="*80 + "\n")
    
    print("ğŸ¯ è°ƒæŸ¥å®Œæˆ!")
    print("\nå…³é”®ä¿®å¤æ­¥éª¤:")
    print("1. æ‰¾åˆ°æ‰€æœ‰è®°å½• 'val/focal_loss' çš„ä½ç½®")
    print("2. åˆå¹¶ä¸ºå•ä¸ªlog_dictè°ƒç”¨")  
    print("3. ç¡®ä¿å‚æ•°ä¸€è‡´")
    print("4. æµ‹è¯•ä¿®å¤ç»“æœ")

if __name__ == "__main__":
    main()