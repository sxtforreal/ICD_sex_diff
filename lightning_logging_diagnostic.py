#!/usr/bin/env python3
"""
PyTorch Lightning é‡å¤æ—¥å¿—é—®é¢˜è¯Šæ–­è„šæœ¬
ç”¨äºåˆ†æå’Œè§£å†³ "You called self.log(val/focal_loss, ...) twice in validation_step with different arguments" é”™è¯¯
"""

import ast
import re
from typing import List, Dict, Tuple, Set
from pathlib import Path

class LightningLoggingDiagnostic:
    def __init__(self, model_file_path: str):
        self.model_file_path = model_file_path
        self.logging_calls = []
        self.duplicate_keys = set()
        
    def analyze_file(self):
        """åˆ†æPythonæ–‡ä»¶ä¸­çš„æ‰€æœ‰loggingè°ƒç”¨"""
        print(f"=== åˆ†ææ–‡ä»¶: {self.model_file_path} ===\n")
        
        try:
            with open(self.model_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except FileNotFoundError:
            print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {self.model_file_path}")
            return
            
        # 1. æŸ¥æ‰¾æ‰€æœ‰ self.log å’Œ self.log_dict è°ƒç”¨
        self._find_logging_calls(content)
        
        # 2. åˆ†æé‡å¤çš„é”®
        self._analyze_duplicate_keys()
        
        # 3. æ£€æŸ¥æ–¹æ³•ä¸­çš„å¤šæ¬¡è°ƒç”¨
        self._check_method_logging_patterns(content)
        
        # 4. æ£€æŸ¥å¯èƒ½çš„æ•°æ®åˆ†å‰²ç›¸å…³é—®é¢˜
        self._check_data_splitting_issues(content)
        
        # 5. æä¾›è§£å†³æ–¹æ¡ˆ
        self._provide_solutions()
        
    def _find_logging_calls(self, content: str):
        """æŸ¥æ‰¾æ‰€æœ‰çš„loggingè°ƒç”¨"""
        print("1. æŸ¥æ‰¾æ‰€æœ‰ logging è°ƒç”¨:")
        print("-" * 40)
        
        # åŒ¹é… self.log å’Œ self.log_dict è°ƒç”¨
        log_pattern = r'self\.log(_dict)?\s*\((.*?)\)'
        matches = re.finditer(log_pattern, content, re.MULTILINE | re.DOTALL)
        
        line_number = 1
        for match in matches:
            # è®¡ç®—è¡Œå·
            lines_before = content[:match.start()].count('\n')
            line_num = lines_before + 1
            
            call_type = "log_dict" if match.group(1) else "log"
            args = match.group(2).strip()
            
            self.logging_calls.append({
                'line': line_num,
                'type': call_type,
                'args': args,
                'full_match': match.group(0)
            })
            
            print(f"ç¬¬ {line_num} è¡Œ: self.{call_type}({args[:100]}{'...' if len(args) > 100 else ''})")
        
        print(f"\næ€»å…±æ‰¾åˆ° {len(self.logging_calls)} ä¸ª logging è°ƒç”¨\n")
        
    def _analyze_duplicate_keys(self):
        """åˆ†æé‡å¤çš„æ—¥å¿—é”®"""
        print("2. åˆ†æé‡å¤çš„æ—¥å¿—é”®:")
        print("-" * 40)
        
        key_locations = {}
        
        for call in self.logging_calls:
            keys = self._extract_keys_from_call(call)
            for key in keys:
                if key not in key_locations:
                    key_locations[key] = []
                key_locations[key].append(call['line'])
        
        # æ‰¾å‡ºé‡å¤çš„é”®
        for key, locations in key_locations.items():
            if len(locations) > 1:
                self.duplicate_keys.add(key)
                print(f"ğŸš¨ é‡å¤é”® '{key}' å‡ºç°åœ¨ç¬¬ {locations} è¡Œ")
        
        if not self.duplicate_keys:
            print("âœ… æœªå‘ç°æ˜æ˜¾çš„é‡å¤é”®")
        print()
        
    def _extract_keys_from_call(self, call: Dict) -> List[str]:
        """ä»loggingè°ƒç”¨ä¸­æå–é”®"""
        keys = []
        args = call['args']
        
        if call['type'] == 'log':
            # self.log("key", value, ...)
            # æå–ç¬¬ä¸€ä¸ªå‚æ•°ä½œä¸ºé”®
            match = re.match(r'["\']([^"\']+)["\']', args.strip())
            if match:
                keys.append(match.group(1))
        else:  # log_dict
            # self.log_dict({...}, ...)
            # å°è¯•æå–å­—å…¸ä¸­çš„é”®
            dict_match = re.search(r'\{([^}]+)\}', args)
            if dict_match:
                dict_content = dict_match.group(1)
                # ç®€å•çš„é”®æå– - æŸ¥æ‰¾ "key": æˆ– 'key': æ¨¡å¼
                key_matches = re.findall(r'["\']([^"\']+)["\']\s*:', dict_content)
                keys.extend(key_matches)
        
        return keys
        
    def _check_method_logging_patterns(self, content: str):
        """æ£€æŸ¥æ–¹æ³•ä¸­çš„loggingæ¨¡å¼"""
        print("3. æ£€æŸ¥æ–¹æ³•ä¸­çš„ logging æ¨¡å¼:")
        print("-" * 40)
        
        # æŸ¥æ‰¾validation_stepå’Œ_stepæ–¹æ³•
        method_patterns = [
            r'def\s+validation_step\s*\([^)]*\):(.*?)(?=def|\Z)',
            r'def\s+_step\s*\([^)]*\):(.*?)(?=def|\Z)',
            r'def\s+training_step\s*\([^)]*\):(.*?)(?=def|\Z)'
        ]
        
        for pattern in method_patterns:
            matches = re.finditer(pattern, content, re.DOTALL)
            for match in matches:
                method_content = match.group(1)
                method_name = re.search(r'def\s+(\w+)', match.group(0)).group(1)
                
                # åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æŸ¥æ‰¾loggingè°ƒç”¨
                method_log_calls = []
                for call in self.logging_calls:
                    call_pos = content.find(call['full_match'])
                    method_start = match.start()
                    method_end = match.end()
                    
                    if method_start <= call_pos <= method_end:
                        method_log_calls.append(call)
                
                if method_log_calls:
                    print(f"æ–¹æ³• '{method_name}' ä¸­çš„ logging è°ƒç”¨:")
                    for call in method_log_calls:
                        print(f"  - ç¬¬ {call['line']} è¡Œ: {call['type']}")
                    print()
        
    def _check_data_splitting_issues(self, content: str):
        """æ£€æŸ¥æ•°æ®åˆ†å‰²ç›¸å…³çš„é—®é¢˜"""
        print("4. æ£€æŸ¥æ•°æ®åˆ†å‰²ç›¸å…³é—®é¢˜:")
        print("-" * 40)
        
        # æŸ¥æ‰¾å¯èƒ½å¯¼è‡´å¤šæ¬¡validationçš„æ¨¡å¼
        patterns_to_check = [
            (r'split.*data', "æ•°æ®åˆ†å‰²ç›¸å…³ä»£ç "),
            (r'multiple.*dataloader', "å¤šä¸ªæ•°æ®åŠ è½½å™¨"),
            (r'val_dataloader.*return', "validation dataloaderè¿”å›"),
            (r'StratifiedKFold|KFold', "äº¤å‰éªŒè¯"),
            (r'for.*fold', "foldå¾ªç¯"),
            (r'self\.current_epoch', "epochç›¸å…³"),
        ]
        
        found_issues = []
        for pattern, description in patterns_to_check:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                found_issues.append((line_num, description, match.group(0)))
        
        if found_issues:
            print("ğŸ” å‘ç°å¯èƒ½ç›¸å…³çš„æ¨¡å¼:")
            for line, desc, match_text in found_issues:
                print(f"  ç¬¬ {line} è¡Œ: {desc} - '{match_text}'")
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾çš„æ•°æ®åˆ†å‰²ç›¸å…³é—®é¢˜")
        print()
        
    def _provide_solutions(self):
        """æä¾›è§£å†³æ–¹æ¡ˆ"""
        print("5. è§£å†³æ–¹æ¡ˆå»ºè®®:")
        print("=" * 50)
        
        if self.duplicate_keys:
            print("ğŸš¨ å‘ç°é‡å¤é”®ï¼Œå»ºè®®ä¿®å¤æ–¹æ¡ˆ:")
            print()
            for key in self.duplicate_keys:
                print(f"å¯¹äºé‡å¤é”® '{key}':")
                print("  æ–¹æ¡ˆ1: åˆå¹¶æ‰€æœ‰ç›¸åŒé”®çš„loggingåˆ°å•ä¸ªlog_dictè°ƒç”¨")
                print("  æ–¹æ¡ˆ2: ç¡®ä¿æ¯ä¸ªé”®åªåœ¨ä¸€ä¸ªåœ°æ–¹è¢«è®°å½•")
                print("  æ–¹æ¡ˆ3: ä½¿ç”¨ä¸åŒçš„é”®ååŒºåˆ†ä¸åŒçš„ç”¨é€”")
                print()
        
        print("é€šç”¨è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥validation_stepæ–¹æ³•ä¸­çš„æ‰€æœ‰self.logå’Œself.log_dictè°ƒç”¨")
        print("2. ç¡®ä¿æ¯ä¸ªmetricé”®åªè¢«è®°å½•ä¸€æ¬¡")
        print("3. ä½¿ç”¨ä¸€è‡´çš„loggingå‚æ•° (sync_dist, prog_barç­‰)")
        print("4. å°†æ‰€æœ‰metricsæ”¶é›†åˆ°å•ä¸ªå­—å…¸ä¸­ï¼Œç„¶åä½¿ç”¨ä¸€æ¬¡log_dict")
        print()
        
        print("æ¨èçš„ä»£ç æ¨¡å¼:")
        print("""
def validation_step(self, batch, batch_idx):
    # æ¨¡å‹å‰å‘ä¼ æ’­
    logits, recon_loss, kl_loss = self(batch)
    
    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    focal_loss = self.focal_loss(logits, labels)
    
    # æ”¶é›†æ‰€æœ‰è¦è®°å½•çš„æŒ‡æ ‡
    metrics = {
        "val/focal_loss": focal_loss,
        "val/recon_loss": recon_loss, 
        "val/kl_loss": kl_loss,
        # å…¶ä»–æŒ‡æ ‡...
    }
    
    # å•æ¬¡è®°å½•æ‰€æœ‰æŒ‡æ ‡
    self.log_dict(metrics, prog_bar=True, sync_dist=True, on_epoch=True)
    
    return focal_loss
        """)

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹"""
    print("PyTorch Lightning é‡å¤æ—¥å¿—é—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 60)
    print()
    
    # è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„model.pyè·¯å¾„
    model_file_path = "/home/sunx/data/aiiih/projects/sunx/projects/SeqSetVAE/main/model.py"
    
    diagnostic = LightningLoggingDiagnostic(model_file_path)
    diagnostic.analyze_file()
    
    print("\n" + "=" * 60)
    print("è¯Šæ–­å®Œæˆ!")
    print("å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·æ£€æŸ¥:")
    print("1. æ˜¯å¦æœ‰å¤šä¸ªè¿›ç¨‹/GPUåœ¨å¹¶è¡Œè®­ç»ƒ")
    print("2. æ˜¯å¦æœ‰è‡ªå®šä¹‰çš„loggingå›è°ƒå‡½æ•°")
    print("3. æ˜¯å¦åœ¨çˆ¶ç±»ä¸­ä¹Ÿæœ‰ç›¸åŒçš„loggingè°ƒç”¨")

if __name__ == "__main__":
    main()